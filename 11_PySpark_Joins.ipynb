{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM7PZqZqhxAcb/fivNrNZY9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/imtheguna/PySpark-Learning/blob/GoogleColab/11_PySpark_Joins.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "miO4_eWislTF",
        "outputId": "fee19951-639e-47e5-db2e-9ca6d21c2efc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "\r0% [Waiting for headers] [Connecting to cloud.r-project.org (18.160.213.101)] [Connecting to ppa.lau\r                                                                                                    \rGet:2 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]\n",
            "\r                                                                                                    \rGet:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]\n",
            "\r0% [2 InRelease 47.5 kB/119 kB 40%] [3 InRelease 46.0 kB/110 kB 42%] [Waiting for headers] [Connecti\r                                                                                                    \rHit:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "\r0% [2 InRelease 47.5 kB/119 kB 40%] [3 InRelease 46.0 kB/110 kB 42%] [Waiting for headers] [Connecti\r0% [2 InRelease 62.0 kB/119 kB 52%] [Waiting for headers] [Connecting to ppa.launchpadcontent.net (1\r                                                                                                    \rHit:5 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:7 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:11 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [1,839 kB]\n",
            "Get:12 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,082 kB]\n",
            "Get:13 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [2,372 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [2,457 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,118 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,374 kB]\n",
            "Fetched 11.5 MB in 4s (2,927 kB/s)\n",
            "Reading package lists... Done\n",
            "sample_data  spark-3.1.1-bin-hadoop3.2\tspark-3.1.1-bin-hadoop3.2.tgz\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=e0730425a25e29bbf3a2c45c53fffd712d5a0780111bbb873b156010e608e84a\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n"
          ]
        }
      ],
      "source": [
        "!apt-get update # Update apt-get repository.\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null # Install Java.\n",
        "!wget -q http://archive.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz # Download Apache Sparks.\n",
        "!tar xf spark-3.1.1-bin-hadoop3.2.tgz # Unzip the tgz file.\n",
        "!pip install -q findspark # Install findspark. Adds PySpark to the System path during runtime.\n",
        "\n",
        "# Set environment variables\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.1-bin-hadoop3.2\"\n",
        "\n",
        "!ls\n",
        "\n",
        "# Initialize findspark\n",
        "import findspark\n",
        "findspark.init()\n",
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "spark = SparkSession \\\n",
        "      .builder \\\n",
        "      .appName('SelectColumns').getOrCreate()\n",
        "\n",
        "\n",
        "df1 = spark.createDataFrame([(1,'Guna'),(2,'Arun'),(3,'Pavi')],['id','name'])\n",
        "\n",
        "df2 = spark.createDataFrame([(1,23),(4,23),(3,30)],['id','age'])\n",
        "\n",
        "df1.show()\n",
        "df2.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pBGB7MCxswJQ",
        "outputId": "fd2b0e5a-ed28-4485-c282-a7c86c93210b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----+\n",
            "| id|name|\n",
            "+---+----+\n",
            "|  1|Guna|\n",
            "|  2|Arun|\n",
            "|  3|Pavi|\n",
            "+---+----+\n",
            "\n",
            "+---+---+\n",
            "| id|age|\n",
            "+---+---+\n",
            "|  1| 23|\n",
            "|  4| 23|\n",
            "|  3| 30|\n",
            "+---+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Inner Join\n",
        "\"\"\"An inner join returns rows from both dataframes that have matching keys.\n",
        "    In other words, it returns only the rows that have common keys in both dataframes. \"\"\"\n",
        "\n",
        "df_inner = df1.join(df2,on=['id'],how='inner')\n",
        "\n",
        "df_inner.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5jr1U3N4tzC_",
        "outputId": "9bcb5774-3503-4053-e157-36860826d49f"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----+---+\n",
            "| id|name|age|\n",
            "+---+----+---+\n",
            "|  1|Guna| 23|\n",
            "|  3|Pavi| 30|\n",
            "+---+----+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Outer Join\n",
        "\"\"\"An outer join, also known as a full join, returns all rows from both dataframes.\n",
        "    If a key is present in one dataframe but not in the other, the missing values are filled with nulls. \"\"\"\n",
        "\n",
        "df_inner = df1.join(df2,on=['id'],how='outer')\n",
        "\n",
        "df_inner.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yj5F1wkp-Fh1",
        "outputId": "2807e2e5-3ec6-4297-90c1-5cc5993aa563"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----+----+\n",
            "| id|name| age|\n",
            "+---+----+----+\n",
            "|  1|Guna|  23|\n",
            "|  3|Pavi|  30|\n",
            "|  2|Arun|null|\n",
            "|  4|null|  23|\n",
            "+---+----+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Left Join\n",
        "\"\"\"A left join returns all rows from the left dataframe and the matched rows from the right dataframe.\n",
        "    If no match is found for a key in the right dataframe, the result will contain null values.\"\"\"\n",
        "\n",
        "df_inner = df1.join(df2,on=['id'],how='left')\n",
        "\n",
        "df_inner.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tlNG_NqR-TzS",
        "outputId": "53454df3-791f-4eea-fa02-a8783c855df8"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----+----+\n",
            "| id|name| age|\n",
            "+---+----+----+\n",
            "|  1|Guna|  23|\n",
            "|  3|Pavi|  30|\n",
            "|  2|Arun|null|\n",
            "+---+----+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Right Join\n",
        "\"\"\"A right join returns all rows from the right dataframe and the matched rows from the left dataframe.\n",
        "  If no match is found for a key in the left dataframe, the result will contain null values.\"\"\"\n",
        "\n",
        "df_inner = df1.join(df2,on=['id'],how='right')\n",
        "\n",
        "df_inner.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TUphwbkD-3N2",
        "outputId": "750a0708-b261-4056-ad93-e068d1d8365f"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----+---+\n",
            "| id|name|age|\n",
            "+---+----+---+\n",
            "|  1|Guna| 23|\n",
            "|  3|Pavi| 30|\n",
            "|  4|null| 23|\n",
            "+---+----+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Left Semi Join\n",
        "\"\"\"A left semi join returns only the columns from the left dataframe for the rows with matching keys in both dataframes.\n",
        "  It is similar to an inner join but only returns the columns from the left dataframe.\"\"\"\n",
        "\n",
        "df_inner = df1.join(df2,on=['id'],how='left_semi')\n",
        "\n",
        "df_inner.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ga649l_a_N-d",
        "outputId": "e26ac0d7-96b5-483d-9fdb-7476980a0a8f"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----+\n",
            "| id|name|\n",
            "+---+----+\n",
            "|  1|Guna|\n",
            "|  3|Pavi|\n",
            "+---+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Left Anti Join\n",
        "\"\"\"A left anti join returns the rows from the left dataframe that do not have matching keys in the right dataframe.\n",
        "  It is the opposite of a left semi join.\"\"\"\n",
        "\n",
        "df_inner = df1.join(df2,on=['id'],how='left_anti')\n",
        "\n",
        "df_inner.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vY_T5R68_Xg4",
        "outputId": "c3471e61-59de-49d3-881b-b523b6beeb04"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----+\n",
            "| id|name|\n",
            "+---+----+\n",
            "|  2|Arun|\n",
            "+---+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Cross Join\n",
        "\"\"\"A cross join, also known as a cartesian join, returns the cartesian product of both dataframes.\n",
        "    It combines each row from the left dataframe with each row from the right dataframe.\"\"\"\n",
        "\n",
        "df_inner = df1.crossJoin(df2)\n",
        "\n",
        "df_inner.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vZv2FGzV_ltm",
        "outputId": "d6019091-65b3-4516-8e79-35702b7111cc"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----+---+---+\n",
            "| id|name| id|age|\n",
            "+---+----+---+---+\n",
            "|  1|Guna|  1| 23|\n",
            "|  1|Guna|  4| 23|\n",
            "|  1|Guna|  3| 30|\n",
            "|  2|Arun|  1| 23|\n",
            "|  3|Pavi|  1| 23|\n",
            "|  2|Arun|  4| 23|\n",
            "|  2|Arun|  3| 30|\n",
            "|  3|Pavi|  4| 23|\n",
            "|  3|Pavi|  3| 30|\n",
            "+---+----+---+---+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}