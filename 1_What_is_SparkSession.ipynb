{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNNmWmbPud81iohYjTYzPJt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/imtheguna/PySpark-Learning/blob/GoogleColab/1_What_is_SparkSession.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **What is SparkSession?**\n",
        "\n",
        "SparkSession is the entry point for any PySpark application, introduced in Spark 2.0 as a unified API to replace the need for separate SparkContext, SQLContext, and HiveContext.\n",
        "\n",
        "# SparkSession offers several benefits that make it an essential component of PySpark applications\n",
        "\n",
        "**Simplified API:** SparkSession unifies the APIs of SparkContext, SQLContext, and HiveContext, making it easier for developers to interact with Spark’s core features without switching between multiple contexts.\n",
        "\n",
        "**Configuration management:** You can easily configure a SparkSession by setting various options, such as the application name, the master URL, and other configurations.\n",
        "\n",
        "**Access to Spark ecosystem: **SparkSession allows you to interact with the broader Spark ecosystem, such as DataFrames, Datasets, and MLlib, enabling you to build powerful data processing pipelines.\n",
        "\n",
        "**Improved code readability:** By encapsulating multiple Spark contexts, SparkSession helps you write cleaner and more maintainable code."
      ],
      "metadata": {
        "id": "BW1LieGpSsST"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update # Update apt-get repository.\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null # Install Java.\n",
        "!wget -q http://archive.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz # Download Apache Sparks.\n",
        "!tar xf spark-3.1.1-bin-hadoop3.2.tgz # Unzip the tgz file.\n",
        "!pip install -q findspark # Install findspark. Adds PySpark to the System path during runtime.\n",
        "\n",
        "# Set environment variables\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.1-bin-hadoop3.2\"\n",
        "\n",
        "!ls\n",
        "\n",
        "# Initialize findspark\n",
        "import findspark\n",
        "findspark.init()\n",
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PWo3-lRpTh7X",
        "outputId": "0e6036a3-ba34-4bfc-9c8f-434e3a358cf9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,622 B]\n",
            "Get:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Hit:3 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:4 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]\n",
            "Get:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [830 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [109 kB]\n",
            "Hit:8 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,069 kB]\n",
            "Get:10 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease [18.1 kB]\n",
            "Hit:11 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,374 kB]\n",
            "Hit:13 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:14 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,082 kB]\n",
            "Get:15 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 Packages [27.8 kB]\n",
            "Get:16 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [1,798 kB]\n",
            "Fetched 7,542 kB in 2s (3,470 kB/s)\n",
            "Reading package lists... Done\n",
            "sample_data  spark-3.1.1-bin-hadoop3.2\tspark-3.1.1-bin-hadoop3.2.tgz\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=fc7b97147fd0cba75ea2137a1cfc32a8659362749071d7488cb4d805b3431e17\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Creating a SparkSession\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"PySpark Application\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .getOrCreate()\n"
      ],
      "metadata": {
        "id": "VqF63BH1TXge"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Accessing SparkSession Components\n",
        "\n",
        "# Access SparkContext\n",
        "spark_context = spark.sparkContext\n",
        "\n",
        "# Access SQLContext\n",
        "sql_context = spark._wrapped\n",
        "\n",
        "# Access HiveContext (if Hive support is enabled)\n",
        "hive_context = spark._jwrapped"
      ],
      "metadata": {
        "id": "Lj0-Tf1LUH4S"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Read CSV File\n",
        "\n",
        "DF = spark.read.csv('/content/data2.csv',header=True,inferSchema=True)\n",
        "\n",
        "DF.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GPM0GZGFUdPx",
        "outputId": "9c9c348f-e449-42e3-d508-23a9e320623b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------------+-------------+-----------+-------------------+----------------+------------------+--------------+----------------------+-------------+---------------+\n",
            "| period|series_reference|  region_name|filled jobs|filled jobs revised|filled jobs diff|filled jobs % diff|total_earnings|total earnings revised|earnings diff|earnings % diff|\n",
            "+-------+----------------+-------------+-----------+-------------------+----------------+------------------+--------------+----------------------+-------------+---------------+\n",
            "|2020.09|     BDCQ.SED1RA|    Northland|      65520|              65904|             384|               0.6|           953|                   959|            6|            0.6|\n",
            "|2020.09|     BDCQ.SED1RB|     Auckland|     708372|             714506|            6134|               0.9|         12420|                 12530|          110|            0.9|\n",
            "|2020.09|     BDCQ.SED1RC|      Waikato|     198776|             200265|            1489|               0.7|          3041|                  3058|           17|            0.6|\n",
            "|2020.09|     BDCQ.SED1RD|Bay of Plenty|     127323|             128540|            1217|               1.0|          1881|                  1899|           18|            1.0|\n",
            "|2020.09|     BDCQ.SED1RE|     Gisborne|      20417|              20632|             215|               1.1|           276|                   279|            3|            1.1|\n",
            "+-------+----------------+-------------+-----------+-------------------+----------------+------------------+--------------+----------------------+-------------+---------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Executing SQL Queries with SparkSession\n",
        "\n",
        "DF.createOrReplaceTempView('Data')\n",
        "\n",
        "df = spark.sql('select * from data limit 1')\n",
        "\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nw9t55TYU9Bk",
        "outputId": "6034d3f7-b957-4b3e-bc7d-ea9dc4c52c97"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------------+-----------+-----------+-------------------+----------------+------------------+--------------+----------------------+-------------+---------------+\n",
            "| period|series_reference|region_name|filled jobs|filled jobs revised|filled jobs diff|filled jobs % diff|total_earnings|total earnings revised|earnings diff|earnings % diff|\n",
            "+-------+----------------+-----------+-----------+-------------------+----------------+------------------+--------------+----------------------+-------------+---------------+\n",
            "|2020.09|     BDCQ.SED1RA|  Northland|      65520|              65904|             384|               0.6|           953|                   959|            6|            0.6|\n",
            "+-------+----------------+-----------+-----------+-------------------+----------------+------------------+--------------+----------------------+-------------+---------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import *\n",
        "## Count Word\n",
        "\n",
        "df = spark.read.text('/content/sample.txt')\n",
        "\n",
        "df = df.select(explode(split(col('value'),' ')).alias('Word'))\n",
        "\n",
        "df = df.groupBy(col('Word')).count()\n",
        "\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qNsmP8LAWNzt",
        "outputId": "45ce038c-69d9-420a-afa7-43c95a1b1a90"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+-----+\n",
            "|        Word|count|\n",
            "+------------+-----+\n",
            "|    Parquet,|    1|\n",
            "|     reading|    1|\n",
            "|         you|    1|\n",
            "|         CSV|    1|\n",
            "|     example|    1|\n",
            "|        read|    1|\n",
            "|      Here’s|    1|\n",
            "|        such|    1|\n",
            "|       file:|    1|\n",
            "|    formats,|    1|\n",
            "|       more.|    1|\n",
            "|        data|    2|\n",
            "|SparkSession|    1|\n",
            "|       Avro,|    1|\n",
            "|        file|    1|\n",
            "|         the|    1|\n",
            "|       write|    1|\n",
            "|     writing|    1|\n",
            "|        from|    1|\n",
            "|         and|    3|\n",
            "+------------+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.stop()"
      ],
      "metadata": {
        "id": "c6dv2G3QXEuQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}