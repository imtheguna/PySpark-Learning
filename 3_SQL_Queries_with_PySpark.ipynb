{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPCxUQ+RG6q9RRiEmZQLMB8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/imtheguna/PySpark-Learning/blob/GoogleColab/3_SQL_Queries_with_PySpark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CDyrRltJhi52",
        "outputId": "19e9bd21-1392-41a1-fa16-4fb04615c99f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]\n",
            "\r0% [Connecting to archive.ubuntu.com (185.125.190.39)] [1 InRelease 12.7 kB/110 kB 12%] [Waiting for\r                                                                                                    \rGet:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,622 B]\n",
            "Get:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]\n",
            "Get:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [830 kB]\n",
            "Hit:7 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "Get:8 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [1,798 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [109 kB]\n",
            "Get:10 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease [18.1 kB]\n",
            "Get:11 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,082 kB]\n",
            "Hit:12 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:13 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,374 kB]\n",
            "Get:15 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 Packages [27.8 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,069 kB]\n",
            "Fetched 7,542 kB in 2s (3,949 kB/s)\n",
            "Reading package lists... Done\n",
            "data2.csv  sample_data\tspark-3.1.1-bin-hadoop3.2  spark-3.1.1-bin-hadoop3.2.tgz\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=60f960b2470a5e371ec5d9c3898480b279e169f973e3f725f166248be8225b66\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n"
          ]
        }
      ],
      "source": [
        "!apt-get update # Update apt-get repository.\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null # Install Java.\n",
        "!wget -q http://archive.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz # Download Apache Sparks.\n",
        "!tar xf spark-3.1.1-bin-hadoop3.2.tgz # Unzip the tgz file.\n",
        "!pip install -q findspark # Install findspark. Adds PySpark to the System path during runtime.\n",
        "\n",
        "# Set environment variables\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.1-bin-hadoop3.2\"\n",
        "\n",
        "!ls\n",
        "\n",
        "# Initialize findspark\n",
        "import findspark\n",
        "findspark.init()\n",
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Running SQL Queries in PySpark\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "df = spark.read \\\n",
        "    .option(\"header\",\"true\") \\\n",
        "    .option(\"inferSchema\",\"true\") \\\n",
        "    .csv('/content/data2.csv')\n",
        "\n",
        "df.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z6J0I2PNpkHG",
        "outputId": "c873f8f6-6741-456c-8975-bd9e54246c67"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------------+-------------+-----------+-------------------+----------------+------------------+--------------+----------------------+-------------+---------------+\n",
            "| period|series_reference|  region_name|filled jobs|filled jobs revised|filled jobs diff|filled jobs % diff|total_earnings|total earnings revised|earnings diff|earnings % diff|\n",
            "+-------+----------------+-------------+-----------+-------------------+----------------+------------------+--------------+----------------------+-------------+---------------+\n",
            "|2020.09|     BDCQ.SED1RA|    Northland|      65520|              65904|             384|               0.6|           953|                   959|            6|            0.6|\n",
            "|2020.09|     BDCQ.SED1RB|     Auckland|     708372|             714506|            6134|               0.9|         12420|                 12530|          110|            0.9|\n",
            "|2020.09|     BDCQ.SED1RC|      Waikato|     198776|             200265|            1489|               0.7|          3041|                  3058|           17|            0.6|\n",
            "|2020.09|     BDCQ.SED1RD|Bay of Plenty|     127323|             128540|            1217|               1.0|          1881|                  1899|           18|            1.0|\n",
            "|2020.09|     BDCQ.SED1RE|     Gisborne|      20417|              20632|             215|               1.1|           276|                   279|            3|            1.1|\n",
            "+-------+----------------+-------------+-----------+-------------------+----------------+------------------+--------------+----------------------+-------------+---------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## We will query the total_earnings for each period\n",
        "\n",
        "query = \"select period,sum(total_earnings) as total_earnings from data group by period;\"\n",
        "\n",
        "df.createOrReplaceTempView('data')\n",
        "\n",
        "df = spark.sql(query)\n",
        "\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ebT9Q1CrpuJ",
        "outputId": "ec82186d-f7ee-40eb-c2e1-a76456c0f301"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+--------------+\n",
            "| period|total_earnings|\n",
            "+-------+--------------+\n",
            "|2022.03|         37128|\n",
            "|2022.12|         40644|\n",
            "|2020.12|         34083|\n",
            "|2021.03|         33830|\n",
            "|2021.06|         34940|\n",
            "|2021.12|         37857|\n",
            "|2022.09|         39158|\n",
            "|2022.06|         38354|\n",
            "|2021.09|         35706|\n",
            "|2020.09|         33588|\n",
            "+-------+--------------+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}